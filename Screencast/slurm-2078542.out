05/16/2024 20:17:58 - INFO - __main__ -   PyTorch: setting up devices
05/16/2024 20:17:58 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: True
05/16/2024 20:17:58 - INFO - __main__ -   Training/evaluation parameters OurTrainingArguments(output_dir='result/RoBERTa-base-uncased-with-synt-50epochs', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=64, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/May16_20-17-58_i01', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=125, dataloader_num_workers=0, past_index=-1, run_name='result/RoBERTa-base-uncased-with-synt-50epochs', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='stsb_spearman', greater_is_better=True, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, eval_transfer=False)
[ERROR|configuration_utils.py:426] 2024-05-16 20:17:58,767 >> Distant resource does not have an ETag, we won't be able to reliably ensure reproducibility.
Traceback (most recent call last):
  File "/home/novikova/miniconda3/envs/contr2/lib/python3.9/site-packages/transformers/configuration_utils.py", line 413, in get_config_dict
    resolved_config_file = cached_path(
  File "/home/novikova/miniconda3/envs/contr2/lib/python3.9/site-packages/transformers/file_utils.py", line 1048, in cached_path
    output_path = get_from_cache(
  File "/home/novikova/miniconda3/envs/contr2/lib/python3.9/site-packages/transformers/file_utils.py", line 1192, in get_from_cache
    raise OSError(
OSError: Distant resource does not have an ETag, we won't be able to reliably ensure reproducibility.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/novikova/contr/SimCSE/train.py", line 592, in <module>
    main()
  File "/home/novikova/contr/SimCSE/train.py", line 332, in main
    config = AutoConfig.from_pretrained(model_args.model_name_or_path, **config_kwargs)
  File "/home/novikova/miniconda3/envs/contr2/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py", line 360, in from_pretrained
    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/home/novikova/miniconda3/envs/contr2/lib/python3.9/site-packages/transformers/configuration_utils.py", line 432, in get_config_dict
    raise EnvironmentError(msg)
OSError: Can't load config for 'RoBERTa-base'. Make sure that:

- 'RoBERTa-base' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'RoBERTa-base' is the correct path to a directory containing a config.json file


