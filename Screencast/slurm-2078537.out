05/16/2024 20:05:42 - INFO - __main__ -   PyTorch: setting up devices
05/16/2024 20:05:42 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: True
05/16/2024 20:05:42 - INFO - __main__ -   Training/evaluation parameters OurTrainingArguments(output_dir='result/bert-base-uncased-with-synt-50epochs', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<EvaluationStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=64, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=50.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/May16_20-05-42_i01', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=125, dataloader_num_workers=0, past_index=-1, run_name='result/bert-base-uncased-with-synt-50epochs', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='stsb_spearman', greater_is_better=True, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, eval_transfer=False)
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 57242 examples [00:00, 133694.74 examples/s]Generating train split: 57242 examples [00:00, 133133.16 examples/s]
[INFO|configuration_utils.py:445] 2024-05-16 20:05:44,978 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/novikova/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:481] 2024-05-16 20:05:44,979 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.2.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|configuration_utils.py:445] 2024-05-16 20:05:45,177 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/novikova/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e
[INFO|configuration_utils.py:481] 2024-05-16 20:05:45,177 >> Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.2.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1766] 2024-05-16 20:05:45,548 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/novikova/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1766] 2024-05-16 20:05:45,549 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/novikova/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4
[INFO|modeling_utils.py:1027] 2024-05-16 20:05:45,784 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/novikova/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f
[WARNING|modeling_utils.py:1134] 2024-05-16 20:05:49,250 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForCL: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']
- This IS expected if you are initializing BertForCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1145] 2024-05-16 20:05:49,250 >> Some weights of BertForCL were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['mlp.dense.weight', 'mlp.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map:   0%|          | 0/57242 [00:00<?, ? examples/s]Map:   2%|▏         | 1000/57242 [00:00<00:05, 9463.70 examples/s]Map:   5%|▌         | 3000/57242 [00:00<00:06, 8139.78 examples/s]Map:   9%|▊         | 5000/57242 [00:00<00:05, 9049.77 examples/s]Map:  12%|█▏        | 7000/57242 [00:00<00:05, 9562.31 examples/s]Map:  14%|█▍        | 8000/57242 [00:00<00:05, 8748.50 examples/s]Map:  17%|█▋        | 10000/57242 [00:01<00:05, 9224.06 examples/s]Map:  21%|██        | 12000/57242 [00:01<00:04, 9604.17 examples/s]Map:  23%|██▎       | 13000/57242 [00:01<00:05, 8798.39 examples/s]Map:  26%|██▌       | 15000/57242 [00:01<00:04, 9245.15 examples/s]Map:  30%|██▉       | 17000/57242 [00:01<00:04, 9564.51 examples/s]Map:  31%|███▏      | 18000/57242 [00:01<00:04, 8864.85 examples/s]Map:  33%|███▎      | 19000/57242 [00:02<00:04, 9032.74 examples/s]Map:  35%|███▍      | 20000/57242 [00:02<00:04, 9141.42 examples/s]Map:  38%|███▊      | 22000/57242 [00:02<00:03, 9556.26 examples/s]Map:  40%|████      | 23000/57242 [00:02<00:03, 8710.78 examples/s]Map:  44%|████▎     | 25000/57242 [00:02<00:03, 9241.54 examples/s]Map:  47%|████▋     | 27000/57242 [00:02<00:03, 9575.46 examples/s]Map:  49%|████▉     | 28000/57242 [00:03<00:03, 8816.44 examples/s]Map:  51%|█████     | 29000/57242 [00:03<00:03, 8772.61 examples/s]Map:  52%|█████▏    | 30000/57242 [00:03<00:03, 8481.46 examples/s]Map:  54%|█████▍    | 31000/57242 [00:03<00:03, 8246.09 examples/s]Map:  56%|█████▌    | 32000/57242 [00:03<00:03, 8231.98 examples/s]Map:  58%|█████▊    | 33000/57242 [00:03<00:03, 7314.35 examples/s]Map:  59%|█████▉    | 34000/57242 [00:03<00:03, 7523.71 examples/s]Map:  61%|██████    | 35000/57242 [00:03<00:02, 7722.63 examples/s]Map:  63%|██████▎   | 36000/57242 [00:04<00:02, 7801.49 examples/s]Map:  65%|██████▍   | 37000/57242 [00:04<00:02, 7574.17 examples/s]Map:  66%|██████▋   | 38000/57242 [00:04<00:02, 6661.79 examples/s]Map:  68%|██████▊   | 39000/57242 [00:04<00:02, 7042.96 examples/s]Map:  70%|██████▉   | 40000/57242 [00:04<00:02, 7351.26 examples/s]Map:  72%|███████▏  | 41000/57242 [00:04<00:02, 7605.50 examples/s]Map:  73%|███████▎  | 42000/57242 [00:04<00:01, 7823.44 examples/s]Map:  75%|███████▌  | 43000/57242 [00:05<00:02, 7006.00 examples/s]Map:  77%|███████▋  | 44000/57242 [00:05<00:01, 7211.53 examples/s]Map:  79%|███████▊  | 45000/57242 [00:05<00:01, 7432.34 examples/s]Map:  80%|████████  | 46000/57242 [00:05<00:01, 7653.73 examples/s]Map:  82%|████████▏ | 47000/57242 [00:05<00:01, 7842.54 examples/s]Map:  84%|████████▍ | 48000/57242 [00:05<00:01, 7041.88 examples/s]Map:  86%|████████▌ | 49000/57242 [00:05<00:01, 7359.10 examples/s]Map:  87%|████████▋ | 50000/57242 [00:06<00:00, 7598.68 examples/s]Map:  89%|████████▉ | 51000/57242 [00:06<00:00, 7700.43 examples/s]Map:  91%|█████████ | 52000/57242 [00:06<00:00, 7860.69 examples/s]Map:  93%|█████████▎| 53000/57242 [00:06<00:00, 6991.09 examples/s]Map:  94%|█████████▍| 54000/57242 [00:06<00:00, 7312.79 examples/s]Map:  96%|█████████▌| 55000/57242 [00:06<00:00, 7591.49 examples/s]Map:  98%|█████████▊| 56000/57242 [00:06<00:00, 7766.12 examples/s]Map: 100%|█████████▉| 57000/57242 [00:06<00:00, 7885.27 examples/s]Map: 100%|██████████| 57242/57242 [00:07<00:00, 8102.21 examples/s]
[INFO|trainer.py:441] 2024-05-16 20:06:16,603 >> The following columns in the training set don't have a corresponding argument in `BertForCL.forward` and have been ignored: .
[INFO|trainer.py:358] 2024-05-16 20:06:16,604 >> Using amp fp16 backend
05/16/2024 20:06:16 - INFO - simcse.trainers -   ***** Running training *****
05/16/2024 20:06:16 - INFO - simcse.trainers -     Num examples = 57242
05/16/2024 20:06:16 - INFO - simcse.trainers -     Num Epochs = 50
05/16/2024 20:06:16 - INFO - simcse.trainers -     Instantaneous batch size per device = 64
05/16/2024 20:06:16 - INFO - simcse.trainers -     Total train batch size (w. parallel, distributed & accumulation) = 64
05/16/2024 20:06:16 - INFO - simcse.trainers -     Gradient Accumulation steps = 1
05/16/2024 20:06:16 - INFO - simcse.trainers -     Total optimization steps = 44750
  0%|          | 0/44750 [00:00<?, ?it/s]/home/novikova/miniconda3/envs/contr2/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
  0%|          | 1/44750 [00:00<10:12:29,  1.22it/s]  0%|          | 2/44750 [00:00<4:57:32,  2.51it/s]   0%|          | 3/44750 [00:01<3:16:59,  3.79it/s]  0%|          | 4/44750 [00:01<2:29:38,  4.98it/s]  0%|          | 5/44750 [00:01<2:25:24,  5.13it/s]  0%|          | 6/44750 [00:01<2:05:09,  5.96it/s]  0%|          | 7/44750 [00:01<1:51:13,  6.71it/s]  0%|          | 8/44750 [00:01<1:42:55,  7.24it/s]  0%|          | 9/44750 [00:01<1:37:27,  7.65it/s]  0%|          | 10/44750 [00:01<1:32:47,  8.04it/s]  0%|          | 11/44750 [00:01<1:30:22,  8.25it/s]  0%|          | 12/44750 [00:02<1:28:50,  8.39it/s]  0%|          | 13/44750 [00:02<1:26:20,  8.64it/s]  0%|          | 14/44750 [00:02<1:25:26,  8.73it/s]  0%|          | 15/44750 [00:02<1:24:11,  8.85it/s]  0%|          | 16/44750 [00:02<1:24:19,  8.84it/s]  0%|          | 17/44750 [00:02<1:24:10,  8.86it/s]  0%|          | 18/44750 [00:02<1:23:21,  8.94it/s]  0%|          | 19/44750 [00:02<1:21:59,  9.09it/s]  0%|          | 20/44750 [00:02<1:21:37,  9.13it/s]  0%|          | 21/44750 [00:03<1:21:20,  9.17it/s]  0%|          | 22/44750 [00:03<1:20:18,  9.28it/s]  0%|          | 23/44750 [00:03<1:20:59,  9.20it/s]slurmstepd: error: *** JOB 2078537 ON i01 CANCELLED AT 2024-05-16T20:06:20 DUE TO TIME LIMIT ***
