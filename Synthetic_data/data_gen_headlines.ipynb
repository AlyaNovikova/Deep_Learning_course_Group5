{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0430a4b5",
   "metadata": {},
   "source": [
    "This notebook presupposes an OpenAI \"clone\" server running on localhost.\n",
    "\n",
    "We used VLLM to create a local server using the Llama3 model. \n",
    "\n",
    "In particular, we used the following terminal command to initialize the server:\n",
    "\n",
    "```python -m vllm.entrypoints.openai.api_server     --model meta-llama/Meta-Llama-3-8B-Instruct --dtype half ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe2f483-1829-46d1-a89b-62a0dcdb0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "from pandas_parallel_apply import DataFrameParallel, SeriesParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2359ef8-74d3-4416-8482-ccc1a3a15406",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcasm_headlines = pd.read_json('Sarcasm_Headlines_Dataset.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aba573a-f66e-451a-82e0-97093dd0ab42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  thirtysomething scientists unveil doomsday clo...   \n",
       "1             0  dem rep. totally nails why congress is falling...   \n",
       "2             0  eat your veggies: 9 deliciously different recipes   \n",
       "3             1  inclement weather prevents liar from getting t...   \n",
       "4             1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                        article_link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarcasm_headlines.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e880520-0639-4f75-b9b5-16f5e9d15c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcasm_headlines['generated_sentence'] = \"No Sentence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "544e0f50-19f5-42dc-95b7-7d9cf022feea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence(row):\n",
    "    if row['generated_sentence'] == 'No Sentence':\n",
    "        example = row['headline']\n",
    "        model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "        openai.api_base = \"http://localhost:8000/v1\"\n",
    "        openai.api_key = \"123\"\n",
    "        model_args={\n",
    "                            \"temperature\": 0.7,\n",
    "                            \"max_tokens\": 4000,\n",
    "                            \"top_p\": 0.95,\n",
    "                            \"frequency_penalty\": 0,\n",
    "                            \"presence_penalty\": 0,\n",
    "                        }\n",
    "        if row['is_sarcastic'] == 1:\n",
    "            messages = [{\"role\": \"user\", \"content\": f\"You are a news headlines writer. Create a sarcastic headline similar to the example. Respond only with the headline. \" + \n",
    "                     f\"Here is an example sentence to help you. Example: {example}\"}]\n",
    "        else:\n",
    "            messages = [{\"role\": \"user\", \"content\": f\"You are a news headlines writer. Create a headline similar to the example. Respond only with the headline. \" + \n",
    "                     f\"Here is an example sentence to help you. Example: {example}\"}]\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                        model=model, messages=messages, request_timeout=120, **model_args\n",
    "                    )\n",
    "            sentence = response['choices'][0]['message']['content']\n",
    "            #print(\"Success:\\n\", sentence)\n",
    "            return sentence\n",
    "        \n",
    "        except Exception as e:  # Exception\n",
    "            print(f\"Error: {e}.\")\n",
    "            if \"response\" in locals():\n",
    "                print(f\"Generated Sentence: {response['choices'][0]['message']['content']}\")\n",
    "    \n",
    "            return \"No Sentence\"\n",
    "    else:\n",
    "       return row['generated_sentence']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f3971e4-b1b0-439a-8424-875c50026a2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process-28: 100%|██████████| 1431/1431 [15:45<00:00,  1.51it/s]\n",
      "Process-33: 100%|██████████| 1431/1431 [15:46<00:00,  1.51it/s]\n",
      "Process-35: 100%|██████████| 1431/1431 [15:47<00:00,  1.51it/s]\n",
      "Process-24: 100%|██████████| 1431/1431 [15:48<00:00,  1.51it/s]\n",
      "Process-26: 100%|██████████| 1431/1431 [15:50<00:00,  1.51it/s]\n",
      "Process-20: 100%|██████████| 1431/1431 [15:50<00:00,  1.51it/s]\n",
      "Process-29: 100%|██████████| 1431/1431 [15:51<00:00,  1.50it/s]\n",
      "Process-27: 100%|██████████| 1431/1431 [15:52<00:00,  1.50it/s]\n",
      "Process-25: 100%|██████████| 1431/1431 [15:53<00:00,  1.50it/s]\n",
      "Process-34: 100%|██████████| 1431/1431 [15:53<00:00,  1.50it/s]\n",
      "Process-37: 100%|██████████| 1431/1431 [15:53<00:00,  1.50it/s]\n",
      "Process-32: 100%|██████████| 1431/1431 [15:54<00:00,  1.50it/s]\n",
      "Process-36: 100%|██████████| 1431/1431 [15:54<00:00,  1.50it/s]\n",
      "Process-39: 100%|██████████| 1430/1430 [15:54<00:00,  1.50it/s]\n",
      "Process-38: 100%|██████████| 1431/1431 [15:54<00:00,  1.50it/s]\n",
      "Process-30: 100%|██████████| 1431/1431 [15:56<00:00,  1.50it/s]\n",
      "Process-31: 100%|██████████| 1431/1431 [15:56<00:00,  1.50it/s]\n",
      "Process-23: 100%|██████████| 1431/1431 [15:57<00:00,  1.49it/s]\n",
      "Process-21: 100%|██████████| 1431/1431 [15:58<00:00,  1.49it/s]\n",
      "Process-22: 100%|██████████| 1431/1431 [15:59<00:00,  1.49it/s]\n"
     ]
    }
   ],
   "source": [
    "sarcasm_headlines['generated_sentence'] = DataFrameParallel(sarcasm_headlines, n_cores = 20, pbar = True).apply(create_sentence, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd604d8b-65b6-46e0-b4fe-b704674b9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_sents = sarcasm_headlines[['generated_sentence','is_sarcastic']]\n",
    "gen_sents.to_csv(\"sarcasm_headlines_synthetic_Llama_3_topp95_temp_7.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
